{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features by using the correlation matrix and elimating columns based on the results and understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='C:/AAKASH/MS NOTES/kaggle/TITANIC/train.csv'\n",
    "raw_trainfile= pd.read_csv(file_path,sep=',')\n",
    "\n",
    "\n",
    "#To compute correlation\n",
    "raw_trainfile = raw_trainfile[['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']]\n",
    "\n",
    "#C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "Embarked_mapping = {'C': 1, 'Q': 2, 'S':3}\n",
    "raw_trainfile['Embarked'] = raw_trainfile['Embarked'].map(Embarked_mapping)\n",
    "\n",
    "gender_mapping = {'male': 1, 'female': 0}\n",
    "raw_trainfile['Sex'] = raw_trainfile['Sex'].map(gender_mapping)\n",
    "\n",
    "corr_matrix=raw_trainfile[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Survived']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the missing values in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_val_train=raw_trainfile.isnull().sum()\n",
    "print(\"BEFORE FILLING:\\n\"+str(miss_val_train))\n",
    "\n",
    "raw_trainfile['Age']=raw_trainfile['Age'].fillna(raw_trainfile['Age'].mean())\n",
    "raw_trainfile['Embarked']=raw_trainfile['Embarked'].fillna(raw_trainfile['Embarked'].mode()[0])\n",
    "\n",
    "miss_val_train=raw_trainfile.isnull().sum()\n",
    "print(\"AFTER FILLING:\\n\"+str(miss_val_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using pandas data frame to import the file. \n",
    "Here in the pandas data frame (df), we cannot use numpy (np) operations directly...we need to convert from df to a numpy array.\n",
    "By importing using pd we dont need to skip the header and index column as after converting to np the header,index is eliminated.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For X_train,We have dropped the 2nd column of targets along with some others using pd and then convert the same into np\n",
    "similarly for Y_train, we have selected only the 2nd column using pd and converted it into np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to implement logistic regression for classifiaction as neural network we need to flatten the data\n",
    "It is done by taking the transpose of the data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block is dedicated to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col=['PassengerId','Survived','SibSp','Name','Ticket','Fare','Cabin']\n",
    "\n",
    "#To calc input matrix\n",
    "X_train =np.array(raw_trainfile.drop(columns=drop_col))\n",
    "print(\"X_train shape:\"+str(X_train.shape))\n",
    "print(\"X_train:\\n\"+str(X_train[:5]))\n",
    "\n",
    "#To calc target matrix\n",
    "Y_train=np.array(raw_trainfile['Survived'])\n",
    "Y_train=Y_train.reshape(-1,1)\n",
    "print(\"Y_train shape:\"+str(Y_train.shape))\n",
    "#print(\"Y_train:\\n\"+str(Y_train[:2]))\n",
    "\n",
    "#To calc flattened input matrix\n",
    "X_train_flat=X_train.T\n",
    "print(\"X_train flattened shape:\"+str(X_train_flat.shape))\n",
    "#print(\"X_train flattened:\\n\"+str(X_train_flat[:2]))\n",
    "\n",
    "#To calc flattened target matrix\n",
    "Y_train_flat=Y_train.T\n",
    "print(\"Y_train flattened shape\"+str(Y_train_flat.shape))\n",
    "#print(\"Y_train flattened\"+str(Y_train_flat[:2]))\n",
    "\n",
    "#To calculate no. of instance,m\n",
    "m=X_train.shape[0]\n",
    "print(\"Training instances:\"+str(m))\n",
    "#to calculate no. of feature,n\n",
    "n=X_train.shape[1]\n",
    "print(\"Training features:\"+str(n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above computations for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='C:/AAKASH/MS NOTES/kaggle/TITANIC/test.csv'\n",
    "raw_testfile= pd.read_csv(file_path,sep=',')\n",
    "#print(\"raw test file:\\n\"+str(raw_testfile[:2]))\n",
    "\n",
    "#C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "Embarked_mapping = {'C': 1, 'Q': 2, 'S':3}\n",
    "raw_testfile['Embarked'] = raw_testfile['Embarked'].map(Embarked_mapping)\n",
    "\n",
    "gender_mapping = {'male': 1, 'female': 0}\n",
    "raw_testfile['Sex'] = raw_testfile['Sex'].map(gender_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_val_test=raw_testfile.isnull().sum()\n",
    "print(\"BEFORE FILLING:\\n\"+str(miss_val_test))\n",
    "\n",
    "raw_testfile['Age']=raw_testfile['Age'].fillna(raw_testfile['Age'].mean())\n",
    "raw_testfile['Embarked']=raw_testfile['Embarked'].fillna(raw_testfile['Embarked'].mode()[0])\n",
    "\n",
    "miss_val_test=raw_testfile.isnull().sum()\n",
    "print(\"AFTER FILLING:\\n\"+str(miss_val_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_col=['PassengerId','SibSp','Name','Ticket','Fare','Cabin']\n",
    "\n",
    "X_test  =np.array(raw_testfile.drop(columns=drop_col))\n",
    "print(\"X_test shape:\"+str(X_test.shape))\n",
    "print(\"X_test:\\n\"+str(X_test[:5]))\n",
    "\n",
    "#Since this project is from kaggle competetion we do not have acces to the tagerts or labels of the test set\n",
    "\n",
    "#To calc flattened input matrix\n",
    "X_test_flat=X_test.T\n",
    "print(\"X_test flattened shape:\"+str(X_test_flat.shape))\n",
    "#print(\"X_test flattened:\\n\"+str(X_test_flat[:2]))\n",
    "\n",
    "\n",
    "#To calculate no. of instance,m\n",
    "m_test=X_test.shape[0]\n",
    "print(\"Test instances:\"+str(m_test))\n",
    "#to calculate no. of feature,n\n",
    "n_test=X_test.shape[1]\n",
    "print(\"Test features:\"+str(n_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing linear function z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(w,b,X):\n",
    "    \n",
    "    Z=np.dot(w.T,X)+b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using Logistic regression as a nueral network setting, we will define the sigmoid function as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    #z is the scalar array\n",
    "    s=1/(1+np.exp(-Z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the parameters w and b as zero vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_parameters(weight_dim):\n",
    "    #weight_dim is the no. of features \n",
    "    w = np.random.randn(weight_dim, 1) * np.sqrt(1 / weight_dim)\n",
    "    #w=w.reshape((weight_dim,1))\n",
    "    b=0\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing forward and backpropagration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_prop(w,b,X,Y):\n",
    "    #m is the no. of training instance\n",
    "    m=X.shape[1]\n",
    "    Z=linear(w,b,X)\n",
    "    A=sigmoid(Z)\n",
    "    delta=1e-20\n",
    "    cost=np.sum((-(Y*np.log(A)))+(-((1-Y)*np.log(1-A+delta))))/m\n",
    "    return cost\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(w,b,X,Y):\n",
    "    #m is the no. of training instance\n",
    "    m=X.shape[1]\n",
    "    Z=linear(w,b,X)\n",
    "    A=sigmoid(Z)\n",
    "    dw=1/m*(np.dot(X,(A-Y).T))\n",
    "    db=1/m*np.sum(A-Y)\n",
    "    \n",
    "    #grads={\"dw\":dw,\n",
    "           #\"db\":db}\n",
    "\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Def a function for optimisation of these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise(w,b,X,Y,iterations,alpha):\n",
    "    J_array=[]\n",
    "    w_array=[]\n",
    "    b_array=[]\n",
    "    J_array.clear()\n",
    "    w_array.clear()\n",
    "    b_array.clear()\n",
    "    min_cost=0\n",
    "    for i in range(iterations):\n",
    "        cost=for_prop(w,b,X,Y)\n",
    "        dw,db=back_prop(w,b,X,Y)\n",
    "        w=w-(alpha*dw)\n",
    "        w_array.append(w)\n",
    "        b=b-(alpha*db)\n",
    "        b_array.append(b)\n",
    "        J_array.append(cost)\n",
    "        if i %2500 == 0:\n",
    "            print(\"iteration\" + str(i)+ \"  : cost\" + str(J_array[i])+\"     :w\"+str(w_array[i]) +\"   :b\"+str(b_array[i]))\n",
    "    plt.plot(J_array)\n",
    "    min_cost=min(J_array)\n",
    "    print(\"min cost\",min_cost)\n",
    "    for i in range(iterations):\n",
    "      if J_array[i]==min_cost:\n",
    "           w=w_array[i]\n",
    "           b=b_array[i]\n",
    "           #print(\"Final w:\"+str(w))\n",
    "           #print(\"Final b:\"+str(b))\n",
    "    return w,b,J_array,w_array,b_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,X):\n",
    "    #m is the no. of training instance\n",
    "    m=X.shape[1]\n",
    "    # initialising the pred matrix with zero\n",
    "    #Y_pred=np.zeros((1,m))\n",
    "    Z=linear(w,b,X)\n",
    "    A=sigmoid(Z)    #shape of (1,m)\n",
    "    Y_pred= (A>=0.5) * 1.0\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merging everything into 1 final function named model to implement the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_model(X_train_flat,X_test_flat,Y_train_flat,iterations,alpha):\n",
    "    #initialise parameters\n",
    "    weight_dim=X_train_flat.shape[0]\n",
    "    w,b=initialise_parameters(weight_dim)\n",
    "    w,b,J_array,w_array,b_array=optimise(w,b,X_train_flat,Y_train_flat,iterations,alpha)\n",
    "    Y_pred_train=predict(w,b,X_train_flat)\n",
    "    Y_pred_test=predict(w,b,X_test_flat)\n",
    "    \n",
    "    Final_pred={\"w\":w,\n",
    "                \"b\":b,\n",
    "                \"J_array\":J_array,\n",
    "                \"w_array\":w_array,\n",
    "                \"b_array\":b_array,\n",
    "                \"Y_pred_train\":Y_pred_train,\n",
    "                \"Y_pred_test\":Y_pred_test}\n",
    "\n",
    "    return Final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally , writing a single line code to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=25000\n",
    "alpha=.001\n",
    "Final_pred=main_model(X_train_flat,X_test_flat,Y_train_flat,iterations,alpha)\n",
    "\n",
    "w=Final_pred[\"w\"]\n",
    "print(\"Final w:\"+str(w))\n",
    "b=Final_pred[\"b\"]\n",
    "print(\"Final b:\"+str(b))\n",
    "\n",
    "Y_pred_train=Final_pred[\"Y_pred_train\"]\n",
    "Y_pred_test=Final_pred[\"Y_pred_test\"]\n",
    "\n",
    "#Training accuracy\n",
    "accuracy = np.mean(Y_pred_train == Y_train_flat)*100\n",
    "print(\"training accuracy\"+str(accuracy))\n",
    "\n",
    "print(\"test predictions\"+str(Y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the prediction values to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=raw_testfile['PassengerId']\n",
    "#print(id.shape)\n",
    "Y_pred_test=Final_pred[\"Y_pred_test\"]\n",
    "#print(Y_pred_test.shape)\n",
    "pred=Y_pred_test.reshape(-1).astype(int)\n",
    "#print(pred.shape)\n",
    "sub_file=pd.DataFrame({'PassengerId':id,'Survived':pred})\n",
    "csv_filepath='C:/AAKASH/MS NOTES/kaggle/TITANIC/submission_file.csv'\n",
    "\n",
    "sub_file.to_csv(csv_filepath,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
